{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 1\n",
    "## Haga un análisis exploratorio de los datos para entenderlos mejor, documente todos los análisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar los datos en un DataFrame de pandas\n",
    "df = pd.read_csv('test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
      "0      1     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "1      0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "2      1     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "3      4     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "4      0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "\n",
      "   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
      "0     0.0  ...       0.0       0.0       0.0       0.0       0.0       0.0   \n",
      "1     0.0  ...       0.0       0.0       0.0       0.0       0.0       0.0   \n",
      "2     0.0  ...       0.0       0.0       0.0       0.0       0.0       0.0   \n",
      "3     0.0  ...       0.0       0.0       0.0       0.0       0.0       0.0   \n",
      "4     0.0  ...       0.0       0.0       0.0       0.0       0.0       0.0   \n",
      "\n",
      "   pixel780  pixel781  pixel782  pixel783  \n",
      "0       0.0       0.0       0.0       0.0  \n",
      "1       0.0       0.0       0.0       0.0  \n",
      "2       0.0       0.0       0.0       0.0  \n",
      "3       0.0       0.0       0.0       0.0  \n",
      "4       0.0       0.0       0.0       0.0  \n",
      "\n",
      "[5 rows x 785 columns]\n"
     ]
    }
   ],
   "source": [
    "# Mostrar las primeras 5 filas de los datos\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 42000 entries, 0 to 41999\n",
      "Columns: 785 entries, label to pixel783\n",
      "dtypes: float64(784), int64(1)\n",
      "memory usage: 251.5 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Mostrar información básica sobre los datos\n",
    "print(df.info())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              label   pixel0   pixel1   pixel2   pixel3   pixel4   pixel5  \\\n",
      "count  42000.000000  28000.0  28000.0  28000.0  28000.0  28000.0  28000.0   \n",
      "mean       4.456643      0.0      0.0      0.0      0.0      0.0      0.0   \n",
      "std        2.887730      0.0      0.0      0.0      0.0      0.0      0.0   \n",
      "min        0.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n",
      "25%        2.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n",
      "50%        4.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n",
      "75%        7.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n",
      "max        9.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n",
      "\n",
      "        pixel6   pixel7   pixel8  ...      pixel774      pixel775  \\\n",
      "count  28000.0  28000.0  28000.0  ...  28000.000000  28000.000000   \n",
      "mean       0.0      0.0      0.0  ...      0.164607      0.073214   \n",
      "std        0.0      0.0      0.0  ...      5.473293      3.616811   \n",
      "min        0.0      0.0      0.0  ...      0.000000      0.000000   \n",
      "25%        0.0      0.0      0.0  ...      0.000000      0.000000   \n",
      "50%        0.0      0.0      0.0  ...      0.000000      0.000000   \n",
      "75%        0.0      0.0      0.0  ...      0.000000      0.000000   \n",
      "max        0.0      0.0      0.0  ...    253.000000    254.000000   \n",
      "\n",
      "           pixel776      pixel777      pixel778  pixel779  pixel780  pixel781  \\\n",
      "count  28000.000000  28000.000000  28000.000000   28000.0   28000.0   28000.0   \n",
      "mean       0.028036      0.011250      0.006536       0.0       0.0       0.0   \n",
      "std        1.813602      1.205211      0.807475       0.0       0.0       0.0   \n",
      "min        0.000000      0.000000      0.000000       0.0       0.0       0.0   \n",
      "25%        0.000000      0.000000      0.000000       0.0       0.0       0.0   \n",
      "50%        0.000000      0.000000      0.000000       0.0       0.0       0.0   \n",
      "75%        0.000000      0.000000      0.000000       0.0       0.0       0.0   \n",
      "max      193.000000    187.000000    119.000000       0.0       0.0       0.0   \n",
      "\n",
      "       pixel782  pixel783  \n",
      "count   28000.0   28000.0  \n",
      "mean        0.0       0.0  \n",
      "std         0.0       0.0  \n",
      "min         0.0       0.0  \n",
      "25%         0.0       0.0  \n",
      "50%         0.0       0.0  \n",
      "75%         0.0       0.0  \n",
      "max         0.0       0.0  \n",
      "\n",
      "[8 rows x 785 columns]\n"
     ]
    }
   ],
   "source": [
    "# Mostrar estadísticas básicas sobre los datos\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Haga un modelo de redes neuronales simple, determine la efectividad del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1313/1313 [==============================] - 6s 4ms/step - loss: 0.2943 - accuracy: 0.9160\n",
      "Epoch 2/10\n",
      "1313/1313 [==============================] - 3s 2ms/step - loss: 0.1299 - accuracy: 0.9623\n",
      "Epoch 3/10\n",
      "1313/1313 [==============================] - 3s 2ms/step - loss: 0.0913 - accuracy: 0.9721\n",
      "Epoch 4/10\n",
      "1313/1313 [==============================] - 3s 2ms/step - loss: 0.0685 - accuracy: 0.9794\n",
      "Epoch 5/10\n",
      "1313/1313 [==============================] - 3s 3ms/step - loss: 0.0514 - accuracy: 0.9840\n",
      "Epoch 6/10\n",
      "1313/1313 [==============================] - 3s 2ms/step - loss: 0.0412 - accuracy: 0.9875\n",
      "Epoch 7/10\n",
      "1313/1313 [==============================] - 3s 2ms/step - loss: 0.0327 - accuracy: 0.9905\n",
      "Epoch 8/10\n",
      "1313/1313 [==============================] - 3s 3ms/step - loss: 0.0257 - accuracy: 0.9921\n",
      "Epoch 9/10\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0204 - accuracy: 0.9939\n",
      "Epoch 10/10\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0162 - accuracy: 0.9952\n",
      "1313/1313 [==============================] - 3s 2ms/step - loss: 0.0119 - accuracy: 0.9971\n",
      "Precisión: 99.71%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Cargar los datos desde el archivo CSV\n",
    "data = pd.read_csv(\"train.csv\")\n",
    "\n",
    "# Dividir los datos en características (X) y etiquetas (y)\n",
    "X = data.iloc[:, 1:].values\n",
    "y = data.iloc[:, 0].values\n",
    "\n",
    "# Normalizar los valores de los píxeles al rango [0, 1]\n",
    "X = X / 255.0\n",
    "\n",
    "# Codificar las etiquetas en un formato one-hot\n",
    "y_one_hot = to_categorical(y)\n",
    "\n",
    "# Crear la red neuronal\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=784, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Entrenar el modelo\n",
    "model.fit(X, y_one_hot, epochs=10, batch_size=32)\n",
    "\n",
    "# Evaluar la precisión del modelo en los datos de entrenamiento\n",
    "loss, accuracy = model.evaluate(X, y_one_hot)\n",
    "print(f\"Precisión: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Haga un modelo de Deep learning, determine la efectividad del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1050/1050 [==============================] - 6s 4ms/step - loss: 1.5497 - accuracy: 0.7975\n",
      "Epoch 2/10\n",
      "1050/1050 [==============================] - 4s 4ms/step - loss: 0.3178 - accuracy: 0.9129\n",
      "Epoch 3/10\n",
      "1050/1050 [==============================] - 4s 3ms/step - loss: 0.2232 - accuracy: 0.9370\n",
      "Epoch 4/10\n",
      "1050/1050 [==============================] - 3s 3ms/step - loss: 0.1940 - accuracy: 0.9464\n",
      "Epoch 5/10\n",
      "1050/1050 [==============================] - 4s 4ms/step - loss: 0.1758 - accuracy: 0.9499\n",
      "Epoch 6/10\n",
      "1050/1050 [==============================] - 4s 4ms/step - loss: 0.1462 - accuracy: 0.9575\n",
      "Epoch 7/10\n",
      "1050/1050 [==============================] - 5s 5ms/step - loss: 0.1279 - accuracy: 0.9632\n",
      "Epoch 8/10\n",
      "1050/1050 [==============================] - 5s 5ms/step - loss: 0.1078 - accuracy: 0.9679\n",
      "Epoch 9/10\n",
      "1050/1050 [==============================] - 4s 4ms/step - loss: 0.1044 - accuracy: 0.9697\n",
      "Epoch 10/10\n",
      "1050/1050 [==============================] - 4s 4ms/step - loss: 0.0826 - accuracy: 0.9755\n",
      "263/263 [==============================] - 1s 3ms/step\n",
      "Accuracy: 95.6786\n"
     ]
    }
   ],
   "source": [
    "# Prepare the data\n",
    "X = data.iloc[:, 1:].values\n",
    "y = data.iloc[:, 0].values\n",
    "y = to_categorical(y)\n",
    "\n",
    "# Split data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Dense(units=128, activation='relu', input_shape=(784,)))\n",
    "model.add(Dense(units=64, activation='relu'))\n",
    "model.add(Dense(units=32, activation='relu'))\n",
    "model.add(Dense(units=10, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred_proba = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "y_test_class = np.argmax(y_test, axis=1)\n",
    "\n",
    "accuracy = accuracy_score(y_test_class, y_pred)\n",
    "print(f\"Accuracy: {accuracy*100:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Haga un modelo con cualquier otro algoritmo que el grupo seleccione, determine la efectividad del modelo\n",
    "### Elegimos CNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar los datos desde archivos CSV\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "# Separar las etiquetas y las imágenes\n",
    "train_labels = train_data.iloc[:, 0].values\n",
    "train_images = train_data.iloc[:, 1:].values\n",
    "test_labels = test_data.iloc[:, 0].values\n",
    "test_images = test_data.iloc[:, 1:].values\n",
    "\n",
    "# Reshape y normalizar las imágenes\n",
    "train_images = train_images.reshape((-1, 28, 28, 1)).astype('float32') / 255\n",
    "test_images = test_images.reshape((-1, 28, 28, 1)).astype('float32') / 255\n",
    "\n",
    "# One-hot encode las etiquetas\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "\n",
    "# Crear el modelo\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "657/657 [==============================] - 30s 44ms/step - loss: 0.2179 - accuracy: 0.9346\n",
      "Epoch 2/10\n",
      "657/657 [==============================] - 26s 39ms/step - loss: 0.0604 - accuracy: 0.9807\n",
      "Epoch 3/10\n",
      "657/657 [==============================] - 25s 38ms/step - loss: 0.0402 - accuracy: 0.9867\n",
      "Epoch 4/10\n",
      "657/657 [==============================] - 25s 38ms/step - loss: 0.0319 - accuracy: 0.9895\n",
      "Epoch 5/10\n",
      "657/657 [==============================] - 21s 32ms/step - loss: 0.0269 - accuracy: 0.9914\n",
      "Epoch 6/10\n",
      "657/657 [==============================] - 24s 37ms/step - loss: 0.0208 - accuracy: 0.9931\n",
      "Epoch 7/10\n",
      "657/657 [==============================] - 26s 39ms/step - loss: 0.0165 - accuracy: 0.9945\n",
      "Epoch 8/10\n",
      "657/657 [==============================] - 25s 38ms/step - loss: 0.0141 - accuracy: 0.9951\n",
      "Epoch 9/10\n",
      "657/657 [==============================] - 27s 42ms/step - loss: 0.0114 - accuracy: 0.9963\n",
      "Epoch 10/10\n",
      "657/657 [==============================] - 26s 40ms/step - loss: 0.0124 - accuracy: 0.9962\n",
      "1313/1313 [==============================] - 11s 8ms/step - loss: nan - accuracy: 0.1018\n",
      "Test accuracy: 10.183333605527878\n"
     ]
    }
   ],
   "source": [
    "# Compilar el modelo\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Entrenar el modelo\n",
    "model.fit(train_images, train_labels, epochs=10, batch_size=64)\n",
    "\n",
    "# Evaluar el modelo\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print('Test accuracy:', test_acc*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3/3 [==============================] - 3s 4ms/step - loss: 0.2720\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.2378\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.2078\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.1810\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.1572\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.1349\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.1177\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.1038\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0936\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0854\n",
      "1/1 [==============================] - 0s 455ms/step\n"
     ]
    }
   ],
   "source": [
    "# Load your dataset or generate some example data\n",
    "# For demonstration purposes, let's generate some random data\n",
    "data = np.random.rand(100, 10)\n",
    "\n",
    "# Split data into features and target\n",
    "X = data[:, :-1]\n",
    "y = data[:, -1]\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Reshape data for LSTM input (samples, time steps, features)\n",
    "X_train_reshaped = X_train_scaled.reshape(X_train_scaled.shape[0], 1, X_train_scaled.shape[1])\n",
    "X_test_reshaped = X_test_scaled.reshape(X_test_scaled.shape[0], 1, X_test_scaled.shape[1])\n",
    "\n",
    "# Create LSTM model\n",
    "model1 = Sequential()\n",
    "model1.add(LSTM(50, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])))\n",
    "model1.add(Dense(1))\n",
    "\n",
    "# Compile model\n",
    "model1.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train model\n",
    "model1.fit(X_train_reshaped, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "# Make predictions\n",
    "predictions1 = model1.predict(X_test_reshaped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2: Stacked LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3/3 [==============================] - 9s 5ms/step - loss: 0.3139\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.2976\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.2815\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.2627\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.2425\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.2192\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1929\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1635\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1334\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1050\n",
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(LSTM(50, return_sequences=True, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])))\n",
    "model2.add(LSTM(50, return_sequences=True))\n",
    "model2.add(LSTM(50))\n",
    "model2.add(Dense(1))\n",
    "\n",
    "# Compile model\n",
    "model2.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train model\n",
    "model2.fit(X_train_reshaped, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "# Make predictions\n",
    "predictions2 = model2.predict(X_test_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Model 1 MSE: 0.08443458362976729\n",
      "Model 2 MSE: 0.08651104842338415\n",
      "Model 1 is more effective.\n"
     ]
    }
   ],
   "source": [
    "# Calculate predictions for both models\n",
    "predictions1 = model1.predict(X_test_reshaped)\n",
    "predictions2 = model2.predict(X_test_reshaped)\n",
    "\n",
    "# Calculate MSE for Model 1\n",
    "mse1 = mean_squared_error(y_test, predictions1)\n",
    "print(f\"Model 1 MSE: {mse1}\")\n",
    "\n",
    "# Calculate MSE for Model 2\n",
    "mse2 = mean_squared_error(y_test, predictions2)\n",
    "print(f\"Model 2 MSE: {mse2}\")\n",
    "\n",
    "# Compare which model is more effective based on MSE\n",
    "if mse1 < mse2:\n",
    "    print(\"Model 1 is more effective.\")\n",
    "else:\n",
    "    print(\"Model 2 is more effective.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
